\section{Introduction}

Reservoir computing (RC) is a form of machine learning that sprung out from the study of recurrent neural networks (RNNs).
It attepts to utilize the temporal dynamics of some fixed complex system, dubbed a 'reservoir',
to preprocess some temporal problem making it separable with a simple linear readout layer.

RNNs, as opposed to feed-forward neural networks,
are notoriously difficult and time-consuming to train.
This due to feedback from the recurrent connections during the training process\cm{RNN training issues}.
It was therefore proposed in both \cite{jaeger2002adaptive} and \cm{original-lsm-paper}
to separate the RNN into two parts, the untrained 'reservoir' part, and the trained 'readout layer'.

It turns out that after imposing certain constrainst on this 'reservoir',
one can efficiently solve many spacial-temporal tasks such as handwriting recognition \cm{handwriting-reservoir}.

The next question then becomes, are there other types of complex systems can be used as reservoirs
and what properties must these reservoirs share to efficiently solve probelms?
Complex networks similar to recurrent neural nets include cellular automata (CAs) and random boolean networks (RBNs), the latter also known as kaufmann or n-k-networks \cm{kaufmann-networks}.
Both are considerably less complex than RNNs, as state transitions can be computed with the help of simple lookup tables, as opposed to the multiplication required in echo-state networks.

Some of these aproaches have allready been successallready been successfully attempted.
In \cite{reservoir-rbn}, the authors combine the reservoir computing approach with random boolean networks, creating a functioning RBN-Reservoir system.
In \cite{fernando2003pattern} the authors successfully implement a RC system using an actual bucket of water as their reservoir.


\todo[inline]{Snakk om paperet til Sipper, vanskeligheten Ã¥ programmere ting \cite{sipper1999emergence}}

\todo[inline]{Snakk om sparsely connected networks, recurrent networks, training difficulty, dynamikk (noe som feedforward neuralnett ikke har), utnytte denne dynamikken, RNN, RBN, CA som alle instanser av komplekse systemer, komplekse nettverk?}

Reservoir computing is a category of machine learning closely related to recurent neural nets (RRN).
As it can be quite difficult to train RRNs, some people \cite{liquidstatepaper} \cite{echostatenetworkpaper} tried only training the last layer of the RNN,
letting the first n-1 layers of the RNN act as a 'reservoir' of dynamics which then could be exited by external input.

Turns out this works, given some constraints on the 'reservoir'.
Elaborate about these constrains, spectral radius < 1 for ESN.

These reservoirs have dynamics which can be analysed.
So if we find other networks with different topologies but similar dynamics,
we should be able to use this new network as the 'reservoir' with a simple readout layer.

If this area of research remains fruitful, it can be used to develop metrics for what other 'reservoirs' or materials might be useful as a  computational base.
This can be useful for evolution in materio, for selecting substrates \cite{evolutionInMaterio}


\subsection{Dynamics and complexity of reservoirs}

So we have all this litterature on complex systems (CITATIONS HERE) that show that optimal computing power is most often found on the 'edge of chaos', or the critical state, on the edge between periodic and chaotic dynamics.
In addition we have a measure for computational power in RRN's developed in \cite{rbn-reservoir} that awards good scores to networks that are able to both separate two at-some-point similar input sequences, as well as two sequences that used to differ in the past.
They show that this measure too, is maximized at critical connectivity (K=2) for RBNs.

\subsection{What i actually done in this paper?}

A rbn-simulator was developed in the Python programming language,
using the following things for glue, regression, inspiration, and duct-tape.

\begin{itemize}
  \item MDP + Oger for putting nodes in a flow, and regressing
  \item Numpy + Scipy for numerical calculations, array backing
  \item RBN Matlab toolbox for inspiration
  \item Homegrown RBN-simulator, Evolutionary alogrithm borrowed from friend
\end{itemize}

To look for what kinds of reservoirs might be useful for computations,
the readout layer from a working RBN-Reservoir+Readout 'flow' was re-used to evolve new RBN-Reservoirs that could re-use this readout layer.

The dynamics of this group of reservoirs is then analysed,
showing whatever similarities there might be between these reservoirs.
