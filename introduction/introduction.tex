\section{Introduction}

Reservoir computing (RC) is a form of machine learning that sprung out from the study of recurrent neural networks (RNNs).
In short, it utilizes the dynamics of some complex system dubbed a 'reservoir' to preprocess a timeseries problem,
transforming it from a temporal to a spacial one in the reservoir, making it then separable with a usually simple readout layer.

In this paper we investigate the dynamics of Reservoir Computing systems where the reservoir is a Random Boolean Network (RBN) \cite{gershenson2004introduction},
an approach found fruitful in \cite{rbn-reservoir}.

First we reproduce chosen experiments from \cite{rbn-reservoir}, creating a working RBN-reservoir computing system (RRC).

Next we investigate whether the readout layer of a working RRC system can be re-used with other RBN-reservoirs than the one it was trained on,
and still stay accurate on the original classification task.
These functionally equivalent reservoirs will be found through the use of an evolutionary algorithm.

Last we look at the dynamics and characteristics of these groups of RBN-Reservoirs, attempting to find any similarities that might be exploitable.

\subsection{A brief introduction to reservoir computing}

Recurrent neural networks, as opposed to feed-forward neural networks,
are notoriously time consuming and difficult to train.
This due to feedback from the recurrent connections during the training process,
allowing small topology changes to drastically change ones position in the fitness landscape.

It was therefore proposed both in \cite{jaeger2002adaptive} (as echo state networks, or ESN)
and \cite{natschlager2002liquid} (as liquid state machines, or LSM) to separate the RNN into two parts,
the untrained reccurrent reservoir, and the trained readout layer.
Both of these methods have been unified into the field of Reservoir Computing,
now focusing on the separate training and evolution of the recurrent and readout part \cite{lukovsevivcius2012reservoir}.

Useful RC resources include Organic \cite{organic}, an online RC hub,
providing documentation, references, and a RC toolbox implemented in python.
It has been quite useful for this project.
Exiting applications of reservoir computing include speech and handwriting recognition,
as well as controlling robotics, as detailed in \cite{lukovsevivcius2012reservoir}.

\subsection{Alternatives to classical reservoirs}

An interesting question arises:
Are there other types of complex systems that can be used as reservoirs?
What properties must these reservoirs have to be able to solve problems?

Complex networks similar to the sparsely connected RNNs found in classical RC devices include Cellular Automata and Random Boolean Networks.
Both are considerably less complex than RNNs, requiring only simple lookup tables for state transitions as opposed to multiplication for RNNs.
A short treatise on Cellular Computing \& friends is available here \cite{sipper1999emergence}.
Cellular computing provides a potentially powerful alternative to classical computers,
leveraging extreme parallelism, simple components and local state.

The ever-so-cited 'Water bucket' paper investigated the use of an actual bucket of water as a reservoir \cite{fernando2003pattern},
successfully recognizing patterns and achieving decent performance at that.
The RBN-reservoir \cite{rbn-reservoir} approach mentioned earlier, has also been found to be viable.

\subsection{Dynamics and complexity of reservoirs}

In general, one wishes to use a reservoir with a complexity matching that of the problem.
Utilizing a modern processor to predict a very simple timeseries would be a classic case of shooting sparrows with cannons \cite{wiki:sparrow},
while achieving the same using a RBN or CA might be more impressive.

\todo[inline]{Gotta finish introduction from hereon and down}

So we have all this litterature on complex systems (CITATIONS HERE) that show that optimal computing power is most often found on the 'edge of chaos', or the critical state, on the edge between periodic and chaotic dynamics.
In addition we have a measure for computational power in RRN's developed in \cite{rbn-reservoir} that awards good scores to networks that are able to both separate two at-some-point similar input sequences, as well as two sequences that used to differ in the past.
They show that this measure too, is maximized at critical connectivity (K=2) for RBNs.

% HERE I WRITTEN TTTO

These reservoirs have dynamics which can be analysed.
So if we find other networks with different topologies but similar dynamics,
we should be able to use this new network as the 'reservoir' with a simple readout layer.

If this area of research remains fruitful,
it can be used to develop metrics for what other 'reservoirs' or even physical materials might be useful as a computational base.
This can be useful for evolution in materio, for selecting substrates \cite{evolutionInMaterio}

\subsection{What i actually done in this paper?}

A rbn-simulator was developed in the Python programming language,
using the following things for glue, regression, inspiration, and duct-tape.

\begin{itemize}
  \item MDP + Oger for putting nodes in a flow, and regressing
  \item Numpy + Scipy for numerical calculations, array backing
  \item RBN Matlab toolbox for inspiration
  \item Homegrown RBN-simulator, Evolutionary alogrithm borrowed from friend
\end{itemize}
