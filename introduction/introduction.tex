\section{Introduction}

Reservoir computing is a category of machine learning closely related to recurent neural nets (RRN).
As it can be quite difficult to train RRNs, some people \cite{liquidstatepaper} \cite{echostatenetworkpaper} tried only training the last layer of the RNN,
letting the first n-1 layers of the RNN act as a 'reservoir' of dynamics which then could be exited by external input.

Turns out this works, given some constraints on the 'reservoir'.
Elaborate about these constrains, spectral radius < 1 for ESN.

These reservoirs have dynamics which can be analysed.
So if we find other networks with different topologies but similar dynamics,
we should be able to use this new network as the 'reservoir' with a simple readout layer.

If this area of research remains fruitful, it can be used to develop metrics for what other 'reservoirs' or materials might be useful as a  computational base.
This can be useful for evolution in materio, for selecting substrates \cite{evolutionInMaterio}


\subsection{A brief introduction to Random boolean networks}

\subsection{RBN-based reservoirs}

Can we use these simple networks for computation?
Turns out Yes!, as shown in this paper \cite{rbn-reservoir}, and it works reasonably well.
Advantages include being MUCH less complex than Echo-state networks and similar, as those require operations such as 'multiplication' which is orders more complex than the simple lookup-table transitions for the RBN nodes.

\subsection{Dynamics and complexity of reservoirs}

So we have all this litterature on complex systems (CITATIONS HERE) that show that optimal computing power is most often found on the 'edge of chaos', or the critical state, on the edge between periodic and chaotic dynamics.
In addition we have a measure for computational power in RRN's developed in \cite{rbn-reservoir} that awards good scores to networks that are able to both separate two at-some-point similar input sequences, as well as two sequences that used to differ in the past.
They show that this measure too, is maximized at critical connectivity (K=2) for RBNs.

\subsection{What i actually done in this paper?}

A rbn-simulator was developed in the Python programming language,
using the following things for glue, regression, inspiration, and duct-tape.

\begin{itemize}
  \item MDP + Oger for putting nodes in a flow, and regressing
  \item Numpy + Scipy for numerical calculations, array backing
  \item RBN Matlab toolbox for inspiration
  \item Homegrown RBN-simulator, Evolutionary alogrithm borrowed from friend
\end{itemize}

To look for what kinds of reservoirs might be useful for computations,
the readout layer from a working RBN-Reservoir+Readout 'flow' was re-used to evolve new RBN-Reservoirs that could re-use this readout layer.

The dynamics of this group of reservoirs is then analysed,
showing whatever similarities there might be between these reservoirs.
