\section{Method}

\todo[inline]{Explain why measurements, and how measurements.}

\subsection{How measurements will be performed}

\subsection{The creation of a RBN}

\subsection{The analyzing of the RRC system}

We will be using a number of computational capability measures to analyze RBNs.
The computational capability measure from \cite{rbn-reservoir} introduced in section \ref{subsection:rbn-reservoir-system} will be used.

$$ computational\_capability = separation - fading\_memory $$

\subsection{The creation of a functioning RRC system}

The training process for a RRC system is threefold.

First, we create a number of datasets for training,
as well as a dataset for later testing.
These datasets are either Temporal Parity or Density,
as described in section \ref{subsection:rbn-reservoir-systems}.

We then either create a new RBN (initialize it randomly),
or load a previously created RBN from disk.

For each bit of input in each dataset,
we perturb the input-connected nodes in the RBN.
After each perturbance, the RBN is ran synchronously (CRBN mode) for one timestep.
The resulting RBN states are collected,
and after the entire dataset is processed,
forwarded to the readout layer.

To find a suitable mapping from the set of reservoir states and the correct input classification,
ridge regression \cite{hoerl1970ridge} is used.
This version of least squares regression is more accurate when faced with input colinearities,
as well as always being at least as accurate as ordinary least squares.

This process is repeated for all the datasets,
and the final regression parameters are chosen as a combination of the parameters obtained for each individual dataset.
Finally we measure the accuracy of the trained reservoir on the test dataset,
obtaining a normalized accuracy score defined as follows.

$$ Accuracy = 1 - \dfrac{sum(actual\_output \neq expected\_output)}{len(correct\_output)} $$.

If the RRC system achieved a high accuracy on a task,
it can then be stored (also known as 'pickled') for further research.

\subsection{The evolving of functionally similar RBNs for existing readout layers}

To investigate the feasibility of evolving RBNs, a genetic algorithm is implemented.

\todo[inline]{Introduction, Fitness function, parent selection and crossover}

We let the genotype be a direct encoding of the corresponding RBN graph.
Such an encoding is significantly larger than a generative encoding,
but less complex as well as able to generate all individuals in the fitness landscape.

Each node needs to represent whether it is connected to the input node,
who its neighbors are, and what its transition rule is.
As we only look at homogenous RBNs,
we can use a fixed-length genome of total length $ n\_nodes \cdot (connectivity + 2) $.

To further simplify GA implementation,
we let all symbols in the genome take a value in the range $[0, max\_required\_in\_genome)$.
The actual symbol values are then computed modulo the largest actual value they could take on
(2 for input connectivity, n\_nodes for neighbors, $2^{2^{n\_nodes}}$ for transition rules).
This redundancy in the genome can be an advantage,
as it allows for new ways to explore the fitness landscape \cm.

The final genome is shown in figure \ref{figure:rbn-genotype}.
Note that this representation sets no limitations on input connectivity or uniqueness of neighbors.

\begin{figure}
  \centering
  \begin{bytefield}[bitwidth=1.5em]{12}
    \bitheader{0-11} \\
    \bitbox{1}{C} \bitbox{1}{N\textsubscript{1}} \bitbox{1}{N\textsubscript{2}} \bitbox{1}{R}
    \bitbox{1}{C} \bitbox{1}{N\textsubscript{1}} \bitbox{1}{N\textsubscript{2}} \bitbox{1}{R}
    \bitbox{1}{C} \bitbox{1}{N\textsubscript{1}} \bitbox{1}{N\textsubscript{2}} \bitbox{1}{R} \\
    \bitbox[t]{4}{$\underbrace{\hspace{6em}}_{\text{\normalsize Node 1}}$}
    \bitbox[t]{4}{$\underbrace{\hspace{6em}}_{\text{\normalsize Node 2}}$}
    \bitbox[t]{4}{$\underbrace{\hspace{6em}}_{\text{\normalsize Node 3}}$} \\
  \end{bytefield}
  \caption{
    The direct-encoded genotype used for evolving RBNs,
    shown here for an RBN with $N=3, K=2$.}
  \label{figure:rbn-genotype}
\end{figure}

\subsection{Find some other place for this?}

It seems that for a random sample of reservoirs of a given size and connetivity,
the temporal density task is solveable for a larger $ n $ than the temporal parity one.
The parity task might therefore requires a more specialized network topology to function well.
Due to the seemingly increased complexity of separating the temporal parity task at low $ n $,
it was chosen as the main task in this paper.

\todo[inline]{Talk a bit about required reservoir complexity, as measured in that previous papaer here?}
\subsection{Measuring dynamics}

\begin{itemize}
  \item
    How do i estimate computational power?
    Use the complexity measure from \cite{rbn-reservoir} of course!
  \item
    How do i estimate transient times and attractor lengths?
    Random samling! Link to some algorithm i use here
  \item
    Fun Anecdote: Link to that paper measuring power usage in RBN-Reservoirs:
    \cite{rbn-reservoir-energy-efficiency}
\end{itemize}

Attractor lengths and such: This might be useful \cite{berdahl2009random}


\subsection{Weaknesses}

\begin{itemize}
  \item Extremely large state-space
  \item Can be difficult to generalize results
  \item
    Genome representation, inefficient, better with binary string?
    Luckily dominated by RBN-Reservoir simulation time
\end{itemize}


\subsection{Experimental setup}

To verify that RBN simulation is working as expected,
a RBN is created randomly, initial state set to all zeros, and ran.
The results are visualized in figure \ref{figure:rbn-noperturb}.
We see that the RBN exhibits stable dynamics, and enters into an attractor around $t=15$.
In figure \ref{figure:rbn-perturb} we continiously perturb the RBN with the input stream from the Temporal Parity task visualized in \ref{figure:temporal-parity}.
In the perturbed case, the state trajectory is continiously changed, preventing the RBN from settling into an attractor.
Interestingly enough, there seems to be a visual similarity between the two cases.
Such a pattern is sure to dissapear with a RBN in the chaotic phase.

This erratic pattern of state transitions is then fed into the readout layer,
which is then tasked with finding a linear combination of the RBN states that results in the expected output for the given task.

\begin{figure}
  \subfloat[Unperturbed]{
    \includegraphics[width=0.5\columnwidth]{method/final-1-noperturb.pdf}
    \label{figure:rbn-noperturb}
  }
  \subfloat[Perturbed]{
    \includegraphics[width=0.5\columnwidth]{method/final-1-perturb.pdf}
    \label{figure:rbn-perturb}
  }
  \caption{
    The same RBN both perturbed and unperturbed.
    Time flows downwards the Y-axis,
    and the current value of each of the nodes in the RBN along the X-axis.
    A cell is white if the value is 1, otherwise black.
    N=100, K=2, P=0.5, L=50, Initial state=0
  }
\end{figure}
