\begin{abstract}

Reservoir Computing, a relatively new approach to machine learning,
utilizes untrained Recurrent Neural Nets as a reservoir of dynamics to preprocess some temporal task,
making it separable with a linear readout layer
Originating from the study of Liquid State machines and Excho State Networks,
potentially any sparsely connected network containing feedforward and feedback loops can be a reservoir.
Random Boolean Networks (RBN) is such a sparsely connected network that may be suitable for Reservoir Computing.

In this paper we investigate the dynamics, performance, and viability of RBNs used for Reservoir Computing (RRC).

Results from a previous paper indicating optimal reservoir input connectivity,
internal RBN connectivity,
and a relationship between Computational Capability and actual performance is reproduced.

Finally, we find a one-to-many mapping between the readout layer in an already-trained RRC system and different RBN reservoirs.
This makes the use of a smaller generative genome for evolving RRC systems interesting.
Even though it hits fewer points in the RBN fitness landscape than a fixed genome,
a large amount of these points are still usable for each instance of a working readout layer.

\end{abstract}
